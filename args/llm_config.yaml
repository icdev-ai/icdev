# [TEMPLATE: CUI // SP-CTI]
# ICDEV LLM Provider Configuration
# Controls which models are used for each function across the platform.
# Single source of truth — replaces scattered hardcoded model IDs.

# -----------------------------------------------------------------------
# Provider Definitions
# -----------------------------------------------------------------------
providers:
  bedrock:
    type: bedrock
    region: ${AWS_DEFAULT_REGION:-us-gov-west-1}
    # Uses boto3 credentials (IAM role, env vars, or ~/.aws/credentials)

  anthropic:
    type: anthropic
    api_key_env: ANTHROPIC_API_KEY
    base_url: https://api.anthropic.com

  openai:
    type: openai_compatible
    api_key_env: OPENAI_API_KEY
    base_url: https://api.openai.com/v1

  ollama:
    type: ollama
    base_url: ${OLLAMA_BASE_URL:-http://localhost:11434}

  gemini:
    type: gemini
    api_key_env: GOOGLE_API_KEY

  vllm:
    type: openai_compatible
    base_url: ${VLLM_BASE_URL:-http://localhost:8000/v1}
    api_key_env: VLLM_API_KEY

  ibm_watsonx:
    type: ibm_watsonx
    api_key_env: IBM_CLOUD_API_KEY
    project_id: ${IBM_WATSONX_PROJECT_ID:-}
    url: ${IBM_WATSONX_URL:-https://us-south.ml.cloud.ibm.com}

# -----------------------------------------------------------------------
# Model Registry
# -----------------------------------------------------------------------
models:
  # -- Anthropic via Bedrock --
  claude-opus:
    provider: bedrock
    model_id: anthropic.claude-opus-4-6-20260215-v1:0
    max_output_tokens: 128000
    supports_thinking: true
    supports_tools: true
    supports_structured_output: true
    supports_vision: true
    pricing: { input_per_1k: 0.005, output_per_1k: 0.025 }

  claude-sonnet:
    provider: bedrock
    model_id: anthropic.claude-sonnet-4-5-20250929-v1:0
    max_output_tokens: 64000
    supports_thinking: true
    supports_tools: true
    supports_structured_output: true
    supports_vision: true
    pricing: { input_per_1k: 0.003, output_per_1k: 0.015 }

  claude-haiku:
    provider: bedrock
    model_id: anthropic.claude-3-5-sonnet-20241022-v2:0
    max_output_tokens: 8192
    supports_thinking: false
    supports_tools: true
    supports_structured_output: false
    pricing: { input_per_1k: 0.003, output_per_1k: 0.015 }

  # -- Anthropic Direct API --
  claude-opus-api:
    provider: anthropic
    model_id: claude-opus-4-6-20260205
    max_output_tokens: 128000
    supports_thinking: true
    supports_tools: true
    supports_structured_output: true
    pricing: { input_per_1k: 0.005, output_per_1k: 0.025 }

  claude-sonnet-api:
    provider: anthropic
    model_id: claude-sonnet-4-5-20250929
    max_output_tokens: 64000
    supports_thinking: true
    supports_tools: true
    supports_structured_output: true
    pricing: { input_per_1k: 0.003, output_per_1k: 0.015 }

  # -- OpenAI --
  gpt-4o:
    provider: openai
    model_id: gpt-4o
    max_output_tokens: 16384
    supports_thinking: false
    supports_tools: true
    supports_structured_output: true
    supports_vision: true
    pricing: { input_per_1k: 0.0025, output_per_1k: 0.010 }

  gpt-4o-mini:
    provider: openai
    model_id: gpt-4o-mini
    max_output_tokens: 16384
    supports_thinking: false
    supports_tools: true
    supports_structured_output: true
    pricing: { input_per_1k: 0.00015, output_per_1k: 0.0006 }

  o3:
    provider: openai
    model_id: o3
    max_output_tokens: 100000
    supports_thinking: true
    supports_tools: true
    supports_structured_output: true
    pricing: { input_per_1k: 0.010, output_per_1k: 0.040 }

  # -- Google Gemini --
  gemini-2.5-pro:
    provider: gemini
    model_id: gemini-2.5-pro-preview-05-06
    max_output_tokens: 65536
    supports_thinking: true
    supports_tools: true
    supports_structured_output: true
    supports_vision: true
    pricing: { input_per_1k: 0.00125, output_per_1k: 0.010 }

  gemini-2.5-flash:
    provider: gemini
    model_id: gemini-2.5-flash-preview-04-17
    max_output_tokens: 65536
    supports_thinking: true
    supports_tools: true
    supports_structured_output: true
    supports_vision: true
    pricing: { input_per_1k: 0.00015, output_per_1k: 0.0006 }

  gemini-2.0-flash:
    provider: gemini
    model_id: gemini-2.0-flash
    max_output_tokens: 8192
    supports_thinking: false
    supports_tools: true
    supports_structured_output: true
    supports_vision: true
    pricing: { input_per_1k: 0.0, output_per_1k: 0.0 }

  # -- Local Models (Ollama) --
  llama-local:
    provider: ollama
    model_id: llama3.3:70b
    max_output_tokens: 8192
    supports_thinking: false
    supports_tools: true
    supports_structured_output: false
    pricing: { input_per_1k: 0.0, output_per_1k: 0.0 }

  codestral-local:
    provider: ollama
    model_id: codestral:22b
    max_output_tokens: 8192
    supports_thinking: false
    supports_tools: false
    supports_structured_output: false
    pricing: { input_per_1k: 0.0, output_per_1k: 0.0 }

  deepseek-local:
    provider: ollama
    model_id: deepseek-r1:70b
    max_output_tokens: 8192
    supports_thinking: true
    supports_tools: false
    supports_structured_output: false
    pricing: { input_per_1k: 0.0, output_per_1k: 0.0 }

  qwen3-local:
    provider: ollama
    model_id: qwen3:latest
    max_output_tokens: 32768
    supports_thinking: true
    supports_tools: true
    supports_structured_output: true
    pricing: { input_per_1k: 0.0, output_per_1k: 0.0 }

  # -- Vision Models (Local) --
  gemma3-local:
    provider: ollama
    model_id: gemma3:latest
    max_output_tokens: 4096
    supports_thinking: false
    supports_tools: false
    supports_structured_output: false
    supports_vision: true
    pricing: { input_per_1k: 0.0, output_per_1k: 0.0 }

  llava-local:
    provider: ollama
    model_id: llava:13b
    max_output_tokens: 4096
    supports_thinking: false
    supports_tools: false
    supports_structured_output: false
    supports_vision: true
    pricing: { input_per_1k: 0.0, output_per_1k: 0.0 }

  # IBM watsonx.ai models (D238)
  granite-code:
    provider: ibm_watsonx
    model_id: ibm/granite-34b-code-instruct
    max_output_tokens: 8192
    supports_thinking: false
    supports_tools: false
    supports_structured_output: false
    pricing:
      input_per_1k: 0.0
      output_per_1k: 0.0
  granite-chat:
    provider: ibm_watsonx
    model_id: ibm/granite-3.1-8b-instruct
    max_output_tokens: 8192
    supports_thinking: false
    supports_tools: true
    supports_structured_output: false
    pricing:
      input_per_1k: 0.0
      output_per_1k: 0.0

# -----------------------------------------------------------------------
# Function -> Model Routing
# -----------------------------------------------------------------------
# Maps each ICDEV function to a model with fallback chain.
# First available model in the chain is used.
routing:
  agent_architect:
    chain: [claude-opus, o3, gemini-2.5-pro, gpt-4o, qwen3-local, llama-local]
    effort: max

  agent_orchestrator:
    chain: [claude-opus, gemini-2.5-pro, gpt-4o, qwen3-local, llama-local]
    effort: high

  agent_builder:
    chain: [claude-opus, gemini-2.5-pro, gpt-4o, qwen3-local, codestral-local]
    effort: max

  agent_compliance:
    chain: [claude-sonnet, gpt-4o, gemini-2.5-flash, qwen3-local, llama-local]
    effort: high

  agent_security:
    chain: [claude-sonnet, gpt-4o, gemini-2.5-flash, qwen3-local, llama-local]
    effort: high

  agent_infra:
    chain: [claude-sonnet, gpt-4o-mini, gemini-2.5-flash, qwen3-local, llama-local]
    effort: medium

  agent_knowledge:
    chain: [claude-sonnet, gpt-4o-mini, gemini-2.5-flash, qwen3-local, llama-local]
    effort: medium

  agent_monitor:
    chain: [claude-haiku, gpt-4o-mini, gemini-2.0-flash, qwen3-local, llama-local]
    effort: low

  task_decomposition:
    chain: [claude-opus, o3, gemini-2.5-pro, gpt-4o, qwen3-local]
    effort: high

  collaboration:
    chain: [claude-sonnet, gpt-4o, gemini-2.5-flash, qwen3-local, llama-local]
    effort: medium

  nlq_sql:
    chain: [gpt-4o-mini, gemini-2.0-flash, claude-haiku, qwen3-local, llama-local]
    effort: low

  code_generation:
    chain: [claude-sonnet, gemini-2.5-pro, gpt-4o, qwen3-local, codestral-local]
    effort: high

  saas_proxy:
    chain: [claude-sonnet, gpt-4o, gemini-2.5-flash, qwen3-local]
    effort: medium

  child_app:
    chain: [claude-sonnet, gemini-2.5-pro, gpt-4o, qwen3-local]
    effort: medium

  screenshot_validation:
    chain: [claude-opus, claude-sonnet, gpt-4o, gemini-2.5-flash, gemma3-local, llava-local]
    effort: medium

  document_vision:
    chain: [claude-sonnet, gpt-4o, gemini-2.5-flash, gemma3-local, llava-local]
    effort: medium

  ui_analysis:
    chain: [claude-opus, claude-sonnet, gpt-4o, gemini-2.5-pro, gemma3-local, llava-local]
    effort: high

  diagram_extraction:
    chain: [claude-opus, claude-sonnet, gpt-4o, gemini-2.5-pro, gemma3-local, llava-local]
    effort: high

  compliance_diagram:
    chain: [claude-sonnet, gpt-4o, gemini-2.5-flash, gemma3-local, llava-local]
    effort: medium

  attachment_analysis:
    chain: [claude-sonnet, gpt-4o-mini, gemini-2.0-flash, gemma3-local, llava-local]
    effort: low

  intake_persona_response:
    chain: [claude-sonnet, claude-opus, gpt-4o, gemini-2.5-flash, qwen3-local, llama-local]
    effort: medium

  bdd_preview:
    chain: [claude-sonnet, gpt-4o, gemini-2.5-flash, qwen3-local, codestral-local]
    effort: low

  narrative_generation:
    chain: [claude-sonnet, gpt-4o, gemini-2.5-flash, qwen3-local, llama-local]
    effort: high

  compliance_export:
    chain: [claude-sonnet, gpt-4o-mini, gemini-2.0-flash, qwen3-local, llama-local]
    effort: medium

  # Cross-Language Translation (Phase 43)
  code_translation:
    chain: [claude-opus, claude-sonnet, o3, gemini-2.5-pro, gpt-4o, qwen3-local, codestral-local]
    effort: max

  code_translation_repair:
    chain: [claude-sonnet, gemini-2.5-pro, gpt-4o, qwen3-local, codestral-local]
    effort: high

  code_translation_review:
    chain: [claude-sonnet, gemini-2.5-pro, gpt-4o, qwen3-local, codestral-local]
    effort: high

  test_translation:
    chain: [claude-sonnet, gemini-2.5-pro, gpt-4o, qwen3-local, codestral-local]
    effort: high

  dependency_suggestion:
    chain: [claude-haiku, gpt-4o-mini, gemini-2.0-flash, qwen3-local, codestral-local]
    effort: low

  default:
    chain: [claude-sonnet, gpt-4o, gemini-2.5-flash, qwen3-local, llama-local]
    effort: medium

# -----------------------------------------------------------------------
# Per-Agent Effort Defaults (Enhancement #4 — used by agent_executor)
# -----------------------------------------------------------------------
agent_effort_defaults:
  orchestrator-agent: high
  architect-agent: max
  builder-agent: max
  compliance-agent: high
  security-agent: high
  infra-agent: medium
  knowledge-agent: medium
  monitor-agent: low
  mbse-agent: high
  modernization-agent: medium
  requirements-agent: medium
  supply-chain-agent: medium
  simulation-agent: high
  devsecops-agent: high

# -----------------------------------------------------------------------
# Embedding Configuration
# -----------------------------------------------------------------------
embeddings:
  default_chain: [openai-embed, gemini-embed, nomic-embed-local]

  models:
    openai-embed:
      provider: openai
      model_id: text-embedding-3-small
      dimensions: 1536
      pricing: { input_per_1k: 0.00002 }

    gemini-embed:
      provider: gemini
      model_id: text-embedding-004
      dimensions: 768
      pricing: { input_per_1k: 0.0 }

    titan-embed:
      provider: bedrock
      model_id: amazon.titan-embed-text-v2:0
      dimensions: 1024
      pricing: { input_per_1k: 0.00002 }

    nomic-embed-local:
      provider: ollama
      model_id: nomic-embed-text
      dimensions: 768
      pricing: { input_per_1k: 0.0 }

    # Azure OpenAI embedding (D238)
    azure-embed:
      provider: azure_openai
      model_id: text-embedding-ada-002
      dimensions: 1536
      pricing:
        input_per_1k: 0.0001
    # OCI GenAI embedding (D238)
    oci-embed:
      provider: oci_genai
      model_id: cohere.embed-english-v3.0
      dimensions: 1024
      pricing:
        input_per_1k: 0.0001
    # IBM watsonx.ai embedding (D238)
    ibm-slate-embed:
      provider: ibm_watsonx
      model_id: ibm/slate-125m-english-rtrvr-v2
      dimensions: 768
      pricing:
        input_per_1k: 0.0

# -----------------------------------------------------------------------
# Global Settings
# -----------------------------------------------------------------------
settings:
  max_retries: 5
  base_retry_delay_seconds: 1.0
  max_retry_delay_seconds: 30.0
  availability_cache_ttl_seconds: 1800
  prefer_local: false    # Two-tier handles local/cloud split — do not set true (blocks Claude tier2)

# -----------------------------------------------------------------------
# Two-Tier Routing (qwen3 worker → Claude planner/reviewer)
# -----------------------------------------------------------------------
# Claude is the brain. qwen3 handles drafting, research, and scanning.
#
# planner_functions : Claude directly — no qwen3 pre-step (architecture,
#                     orchestration, high-stakes planning)
# worker_functions  : qwen3 drafts a COMPACT structured response, then
#                     Claude verifies/corrects and returns the final answer.
#                     qwen3 output is intentionally short (reduces Claude's
#                     input tokens vs. Claude doing the full task alone).
# scanner_functions : qwen3 only — log analysis, health checks, simple
#                     pattern matching where review adds no value.
#
# Set enabled: false to fall back to normal chain-based routing.
two_tier:
  enabled: true
  tier1_model: qwen3-local    # Worker — research, draft, scan (free/local)
  tier2_model: claude-sonnet  # Planner/Reviewer — verify, decide, plan

  planner_functions:
    - agent_architect
    - agent_orchestrator
    - task_decomposition
    - child_app
    - saas_proxy

  worker_functions:
    - agent_builder
    - agent_compliance
    - agent_security
    - agent_infra
    - agent_knowledge
    - code_generation
    - code_translation
    - code_translation_repair
    - code_translation_review
    - test_translation
    - bdd_preview
    - nlq_sql
    - narrative_generation
    - compliance_export
    - intake_persona_response
    - collaboration
    - dependency_suggestion
    - default

  scanner_functions:
    - agent_monitor
    - attachment_analysis

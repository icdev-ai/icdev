#!/usr/bin/env python3
# CUI // SP-CTI
# Controlled by: Department of Defense
# CUI Category: CTI
# Distribution: D
# POC: ICDEV System Administrator
"""NIST AI RMF Assessor â€” AI Risk Management Framework assessment.

Assesses NIST AI Risk Management Framework 1.0 compliance across
4 core functions: Govern, Map, Measure, Manage.
Pattern: tools/compliance/base_assessor.py (BaseAssessor ABC).

Usage:
    python tools/compliance/nist_ai_rmf_assessor.py --project-id proj-123
    python tools/compliance/nist_ai_rmf_assessor.py --project-id proj-123 --gate
    python tools/compliance/nist_ai_rmf_assessor.py --project-id proj-123 --json
"""

import sys
from pathlib import Path
from typing import Dict, Optional

sys.path.insert(0, str(Path(__file__).resolve().parent))
from base_assessor import BaseAssessor


class NISTAIRMFAssessor(BaseAssessor):
    FRAMEWORK_ID = "nist_ai_rmf"
    FRAMEWORK_NAME = "NIST AI Risk Management Framework 1.0"
    TABLE_NAME = "nist_ai_rmf_assessments"
    CATALOG_FILENAME = "nist_ai_rmf.json"

    def get_automated_checks(
        self, project: Dict, project_dir: Optional[str] = None,
    ) -> Dict[str, str]:
        """NIST AI RMF automated checks.

        Checks for:
        - GOVERN-1: AI governance policies (security_gates.yaml or AI policy docs)
        - GOVERN-2: AI risk management accountability (ai_telemetry or agent config)
        - MAP-1: AI system context documented (CLAUDE.md or project docs)
        - MAP-2: AI categorization performed (FIPS 199 categorization exists)
        - MAP-3: AI-specific risks identified (risk register or ATLAS assessments)
        - MEASURE-1: AI system monitored (ai_telemetry or monitoring config)
        - MEASURE-2: AI trustworthiness assessed (assessment reports or test results)
        - MEASURE-3: AI token usage tracked (agent_token_usage records)
        - MANAGE-1: AI risks managed (prompt_injection_detector or input validation)
        - MANAGE-2: AI incidents responded to (incident_response or auto_resolver)
        """
        results = {}

        if not project_dir:
            return results

        project_path = Path(project_dir)
        has_ai_policy = False
        has_ai_accountability = False
        has_ai_context = False
        has_ai_monitoring = False
        has_ai_security = False
        has_incident_response = False

        # Check for AI governance policies (GOVERN-1)
        for yaml_file in project_path.rglob("*.yaml"):
            try:
                content = yaml_file.read_text(encoding="utf-8", errors="ignore")
                lower = content.lower()
                if "ai" in lower and ("policy" in lower or "governance" in lower or "security_gates" in lower):
                    has_ai_policy = True
                if "ai" in lower and ("accountab" in lower or "telemetry" in lower):
                    has_ai_accountability = True
                if "monitor" in lower and ("ai" in lower or "model" in lower or "agent" in lower):
                    has_ai_monitoring = True
            except Exception:
                continue

        # Check for AI context documentation (MAP-1) and security controls (MANAGE-1)
        for py_file in project_path.rglob("*.py"):
            try:
                content = py_file.read_text(encoding="utf-8", errors="ignore")
                lower = content.lower()
                if "prompt" in lower and ("inject" in lower or "detect" in lower or "valid" in lower):
                    has_ai_security = True
                if "incident" in lower and "response" in lower:
                    has_incident_response = True
            except Exception:
                continue

        # Check for CLAUDE.md or AI documentation (MAP-1)
        if (project_path / "CLAUDE.md").exists() or (project_path / "docs").is_dir():
            has_ai_context = True

        if has_ai_policy:
            results["GOVERN-1"] = "satisfied"
        if has_ai_accountability:
            results["GOVERN-2"] = "satisfied"
        if has_ai_context:
            results["MAP-1"] = "satisfied"
        if has_ai_monitoring:
            results["MEASURE-1"] = "satisfied"
        if has_ai_security:
            results["MANAGE-1"] = "satisfied"
        if has_incident_response:
            results["MANAGE-2"] = "satisfied"

        return results


if __name__ == "__main__":
    NISTAIRMFAssessor().run_cli()
